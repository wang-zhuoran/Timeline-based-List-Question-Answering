{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 1071 examples [00:00, 89685.94 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# read in the predictions and test data from json files\n",
    "# read the json files D:\\Study\\q4\\NLP\\Timeline-based-List-Question-Answering\\data\\test_TLQA.json\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def read_json(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import re\n",
    "\n",
    "test_data = load_dataset('json', data_files='data/test_TLQA.json')\n",
    "# predictions = load_dataset('json', data_files='mid-output-update/results/onlyQA_predictions-FlanT5-large-k=10.json')\n",
    "\n",
    "predictions = load_dataset('json', data_files='mid-output-update/results/onlyQA_predictions-FlanT5-xl-k=10.json')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['Birmingham City F.C.']}\n",
      "\n",
      "\n",
      "{'text': ['Answer: Manchester City F.C.']}\n",
      "\n",
      "\n",
      "<class 'datasets.dataset_dict.DatasetDict'>\n"
     ]
    }
   ],
   "source": [
    "# print example predictions data and test data\n",
    "\n",
    "def print_sample_data(dataset, num_samples=1):\n",
    "    print(dataset['train'][:num_samples])\n",
    "    print('\\n')\n",
    "\n",
    "print_sample_data(test_data, num_samples=1)\n",
    "print_sample_data(predictions, num_samples=1)\n",
    "\n",
    "print(type(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['text'])\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 1071\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(test_data['train'][0].keys())\n",
    "\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the entity match\n",
    "# Define metric functions\n",
    "def entity_match(predictions, references):\n",
    "    def extract_entity(text):\n",
    "        # Remove \"Answer:\" prefix if present\n",
    "        if text.lower().startswith(\"answer:\"):\n",
    "            text = text[len(\"answer:\"):].strip()\n",
    "        # Extract the entity part before any timeline information\n",
    "        entity = re.sub(r'\\s*\\(.*?\\)', '', text).strip().lower()\n",
    "        return entity\n",
    "\n",
    "    pred_entities = [extract_entity(pred) for pred in predictions]\n",
    "    ref_entities = [extract_entity(ref) for ref in references]\n",
    "    \n",
    "    # Convert lists to sets\n",
    "    pred_entities_set = set(pred_entities)\n",
    "    ref_entities_set = set(ref_entities)\n",
    "    \n",
    "    # Calculate matches\n",
    "    matches = len(pred_entities_set & ref_entities_set)  # Intersection of sets\n",
    "    \n",
    "    return matches / len(ref_entities_set) if ref_entities_set else 0\n",
    "\n",
    "test_references = [entry['text'] for entry in test_data['train']]\n",
    "predictions = [entry['text'] for entry in predictions['train']]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "def timeline_match(predictions, references):\n",
    "    def extract_years(text):\n",
    "        matches = re.findall(r'\\d{4}', text)\n",
    "        if matches:\n",
    "            return set(map(int, matches))\n",
    "        return set()\n",
    "\n",
    "    match_scores = []\n",
    "\n",
    "    for pred, ref in zip(predictions, references):\n",
    "        ref_years = extract_years(ref)\n",
    "        pred_years = extract_years(pred)\n",
    "\n",
    "        matches = len(pred_years & ref_years)\n",
    "        match_scores.append(matches / len(ref_years) if ref_years else 0)\n",
    "\n",
    "    return sum(match_scores) / len(match_scores) if match_scores else 0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_metric(predictions, references):\n",
    "    def compute_f1(pred_list, ref_list):\n",
    "        pred_tokens = set(pred_list)\n",
    "        ref_tokens = set(ref_list)\n",
    "\n",
    "        if not ref_tokens:\n",
    "            return 0.0\n",
    "\n",
    "        true_positives = len(pred_tokens & ref_tokens)\n",
    "        precision = true_positives / len(pred_tokens) if pred_tokens else 0\n",
    "        recall = true_positives / len(ref_tokens) if ref_tokens else 0\n",
    "\n",
    "        if precision + recall == 0:\n",
    "            return 0.0\n",
    "\n",
    "        return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    f1_scores = []\n",
    "\n",
    "    for pred, ref in zip(predictions, references):\n",
    "        # Flatten the lists for token-level comparison\n",
    "        all_preds = [token for token in re.findall(r'\\w+', pred)]\n",
    "        all_refs = [token for token in re.findall(r'\\w+', ref)]\n",
    "        f1_scores.append(compute_f1(all_preds, all_refs))\n",
    "\n",
    "    return sum(f1_scores) / len(f1_scores) if f1_scores else 0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_years(text):\n",
    "    matches = re.findall(r'\\d{4}', text)\n",
    "    if matches:\n",
    "        years = list(map(int, matches))\n",
    "        return min(years), max(years) \n",
    "    return None, None\n",
    "\n",
    "\n",
    "def time_metric(predictions, references):\n",
    "    time_diffs = []\n",
    "\n",
    "    for pred, ref in zip(predictions, references):\n",
    "        pred_start, pred_end = extract_years(pred)\n",
    "        ref_start, ref_end = extract_years(ref)\n",
    "\n",
    "        # Debugging prints\n",
    "        # print(f\"Prediction: {pred}, Extracted Years: ({pred_start}, {pred_end})\")\n",
    "        # print(f\"Reference: {ref}, Extracted Years: ({ref_start}, {ref_end})\")\n",
    "\n",
    "        if pred_start is not None and ref_start is not None:\n",
    "            time_diff = abs(pred_start - ref_start) + abs(pred_end - ref_end)\n",
    "            time_diffs.append(time_diff)\n",
    "        else:\n",
    "            time_diffs.append(float('inf'))  \n",
    "\n",
    "    valid_diffs = [diff for diff in time_diffs if diff != float('inf')]\n",
    "    if not valid_diffs:\n",
    "        return float('inf') \n",
    "    return sum(valid_diffs) / len(valid_diffs)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def completeness(predictions, references):\n",
    "    scores = []\n",
    "    \n",
    "    for pred_list, ref_list in zip(predictions, references):\n",
    "        pred_items = set([item.strip().lower() for pred in pred_list for item in pred.split(\", \")])\n",
    "        ref_items = set([item.strip().lower() for ref in ref_list for item in ref.split(\", \")])\n",
    "        \n",
    "        correct_count = len(pred_items.intersection(ref_items))\n",
    "        total_count = len(ref_items)\n",
    "        \n",
    "        scores.append(correct_count / total_count if total_count > 0 else 0)\n",
    "    return sum(scores) / len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity Match Score: 0.07113543091655267\n",
      "Timeline Match Score: 0.2217353944944981\n",
      "F1 Score: 0.16831609640270248\n",
      "Time Metric: 6.419064748201439\n",
      "Completeness Score: 0.5779583311043701\n"
     ]
    }
   ],
   "source": [
    "entity_match_score = entity_match(predictions, test_references)\n",
    "print(f\"Entity Match Score: {entity_match_score}\")\n",
    "timeline_match_score = timeline_match(predictions, test_references)\n",
    "print(f\"Timeline Match Score: {timeline_match_score}\")\n",
    "f1_score_value = f1_metric(predictions, test_references)\n",
    "print(f\"F1 Score: {f1_score_value}\")\n",
    "time_metric_value = time_metric(predictions, test_references)\n",
    "print(f\"Time Metric: {time_metric_value}\")\n",
    "completeness_score = completeness(predictions, test_references)\n",
    "print(f\"Completeness Score: {completeness_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
